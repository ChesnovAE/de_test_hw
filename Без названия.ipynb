{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "from pyspark import SparkContext, SparkConf, HiveContext\n",
    "from pyspark.sql import SparkSession, Row, functions, Column\n",
    "from pyspark.sql.types import StringType \n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "# pd.set_option('display.max_columns',  30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_config(app_name,\n",
    "                  memory=1,\n",
    "                  instances=1,\n",
    "                  cores=2,\n",
    "                  max_result_size=50,\n",
    "                  driver_memory=1):\n",
    "    \"\"\"\n",
    "    simple wrapper for SparkConfig().set()\n",
    "    :param memory: int, max provided memory, in gigabytes\n",
    "    :param cores: int, max cores\n",
    "    :return: pyspark.conf.SparkConf object\n",
    "    \"\"\"\n",
    "    spark_conf = SparkConf().setAppName(app_name)\n",
    "#     spark_conf.set('spark.executor.memory', f'{memory}g')\n",
    "#     spark_conf.set('spark.executor.instances', f'{instances}')\n",
    "#     spark_conf.set('spark.executor.cores', f'{cores}')\n",
    "#     spark_conf.set('spark.driver.memory', f'{driver_memory}g')\n",
    "#     spark_conf.set(\"spark.executor.allowSparkContext\", 'true')\n",
    "#     spark_conf.set(\"spark.sql.sources.partitionOverwriteMode\", \"dynamic\")\n",
    "#     spark_conf.set('hive.exec.dynamic.partition.mode','nonstrict')\n",
    "    return spark_conf\n",
    "\n",
    "def create_session(conf,):\n",
    "    return SparkSession.Builder().config(conf=conf).getOrCreate()\n",
    "\n",
    "ses = create_session(create_config('tmp_writer'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://194-67-111-68.cloudvps.regruhosting.ru:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>tmp_writer</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=tmp_writer>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ses.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = ses.sparkContext.textFile('admin_lev6.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file.map(lambda x: x.split(';'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = file.take(1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = file.filter(lambda x: x != cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ses.createDataFrame(file, schema=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+-------------------+---------+--------+------+---------+--------+---------+--------+---------+--------------------+---------+-----------------+---------+--------+---------+--------+---------+--------+---------+--------+---------+--------+----------+---------+\n",
      "|               NAME|      NAME_EN|            NAME_RU|ADMIN_LVL|OSM_TYPE|OSM_ID|ADMIN_L1D|ADMIN_L1|ADMIN_L2D|ADMIN_L2|ADMIN_L3D|            ADMIN_L3|ADMIN_L4D|         ADMIN_L4|ADMIN_L5D|ADMIN_L5|ADMIN_L6D|ADMIN_L6|ADMIN_L7D|ADMIN_L7|ADMIN_L8D|ADMIN_L8|ADMIN_L9D|ADMIN_L9|ADMIN_L10D|ADMIN_L10|\n",
      "+-------------------+-------------+-------------------+---------+--------+------+---------+--------+---------+--------+---------+--------------------+---------+-----------------+---------+--------+---------+--------+---------+--------+---------+--------+---------+--------+----------+---------+\n",
      "|      Чунский район|Chunsky Rayon|      Чунский район|        6|relation|190097|         |        |    60189|  Россия|  1221148|Сибирский федерал...|   145454|Иркутская область|         |        |         |        |         |        |         |        |         |        |          |         |\n",
      "|Усть-Илимский район|  Ust-Ilimsky|Усть-Илимский район|        6|relation|190095|         |        |    60189|  Россия|  1221148|Сибирский федерал...|   145454|Иркутская область|         |        |         |        |         |        |         |        |         |        |          |         |\n",
      "+-------------------+-------------+-------------------+---------+--------+------+---------+--------+---------+--------+---------+--------------------+---------+-----------------+---------+--------+---------+--------+---------+--------+---------+--------+---------+--------+----------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = pd.read_csv('admin_lev8.txt', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = ses.read.options(delimiter=';', header=True).format('csv').load('admin_lev8.txt')\n",
    "df = ses.createDataFrame(d.astype(str))\n",
    "# # df_p = df.toPandas()\n",
    "\n",
    "def last_nonnull_func(osm_ids):\n",
    "    return '2'\n",
    "\n",
    "# udf_cols = [F.col(f'ADMIN_L{i}D') for i in range(1, 11)]\n",
    "\n",
    "last_nonnull_udf = F.udf(lambda osm_ids: last_nonnull_udf(osm_ids), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-7-2ed79d06756c>\", line 10, in <lambda>\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/udf.py\", line 199, in wrapper\n    return self(*args)\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 177, in __call__\n    judf = self._judf\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 161, in _judf\n    self._judf_placeholder = self._create_judf()\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 167, in _create_judf\n    spark = SparkSession.builder.getOrCreate()\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/session.py\", line 228, in getOrCreate\n    sc = SparkContext.getOrCreate(sparkConf)\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/context.py\", line 384, in getOrCreate\n    SparkContext(conf=conf or SparkConf())\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/context.py\", line 136, in __init__\n    SparkContext._assert_on_driver()\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/context.py\", line 1277, in _assert_on_driver\n    raise Exception(\"SparkContext should only be created and accessed on the driver.\")\nException: SparkContext should only be created and accessed on the driver.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-21ac3d52544d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'par_osm_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_nonnull_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NAME'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \"\"\"\n\u001b[1;32m    483\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mtruncate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 484\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 604, in main\n    process()\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 596, in process\n    serializer.dump_stream(out_iter, outfile)\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 211, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 132, in dump_stream\n    for obj in iterator:\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/serializers.py\", line 200, in _batched\n    for item in iterator:\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in mapper\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 450, in <genexpr>\n    result = tuple(f(*[a[o] for o in arg_offsets]) for (arg_offsets, f) in udfs)\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/worker.py\", line 85, in <lambda>\n    return lambda *a: f(*a)\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/util.py\", line 73, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-7-2ed79d06756c>\", line 10, in <lambda>\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/pyspark/sql/udf.py\", line 199, in wrapper\n    return self(*args)\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 177, in __call__\n    judf = self._judf\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 161, in _judf\n    self._judf_placeholder = self._create_judf()\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/udf.py\", line 167, in _create_judf\n    spark = SparkSession.builder.getOrCreate()\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/sql/session.py\", line 228, in getOrCreate\n    sc = SparkContext.getOrCreate(sparkConf)\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/context.py\", line 384, in getOrCreate\n    SparkContext(conf=conf or SparkConf())\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/context.py\", line 136, in __init__\n    SparkContext._assert_on_driver()\n  File \"/home/ripper/spark-3.1.1-bin-hadoop2.7/python/lib/pyspark.zip/pyspark/context.py\", line 1277, in _assert_on_driver\n    raise Exception(\"SparkContext should only be created and accessed on the driver.\")\nException: SparkContext should only be created and accessed on the driver.\n"
     ]
    }
   ],
   "source": [
    "df.withColumn('par_osm_id', last_nonnull_udf(F.col('NAME'))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
